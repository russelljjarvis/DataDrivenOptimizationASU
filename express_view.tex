\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{enumitem}


\renewcommand{\labelitemi}{$\bullet$}
\renewcommand{\labelitemii}{$\cdot$}
\renewcommand{\labelitemiii}{$\diamond$}
\renewcommand{\labelitemiv}{$\ast$}
\begin{document}
	
	
%-1	\part{part}
%0	\chapter{chapter}
%1	\section{section}
%2	\subsection{subsection}
%3	\subsubsection{subsubsection}
%4	\paragraph{paragraph}
%5	\subparagraph{subparagraph}

\chapter*{General Introduction}

\item Also, you will want both a label general introduction and label general discussion, but also within each chapter you will need a mini-introduction that explains what you are aiming to do or show in that chapter and why. 

\item That mini-introduction can be essentially like the content of the first few cells of some of your notebooks, but you will want to make sure that every sentence is comprehensible to someone who has read and understood the general introduction but otherwise doesnâ€™t have much special expertise other than being a neuroscientists of some kind.


% Explanatory comment Name of section, above call to include.
Need to explain two things:
- why optimize, but also why reduced models. Probably why Reduced models first.
\subsection*{Motivation for Reduced Models of electrically excitable cortical cells}

\item[-] Biophysically accurate models take a significant time and resources to evaluate. A different class of neuronal model, known as a "Reduced model" is comparatively fast to solve especially, when many models are required to be simulated simultaneously. Some of examples of reduced models are: 
\subitem AdExp, Izhi, GLIF

\subsection*{speed of simulation is important for learning about the brain}. Digital modeling of physical properties of cells is occurring at the mesoscopic scale, however adding microscopic features to simulations significantly increases simulation time. Even when using High Performance Computing developing simulations involves a model error checking phase. Model development development is faster when simulation output occurs sooner.\\
\\
The are several valid instances when the complete three dimensional form of a neuron is an integral part of a brain simulation, such as in the Blue Brain somato-sensory cortex model \cite{} and the Allen Institute $V1$ model \cite{}, These simulations are improved by encasing a "core" of biophysically accurate models inside a "shell" of simple fast and reduced Izhi, GLIF, or AdExp models. 

Encasing a complex core inside a shell of simplified models mitigates an observable "edge effect" problem. The problem is that simulations concern sub divisions of brain tissue, subdivisions by nature exclude externally sourced synaptic inputs. These synaptic inputs are connections that are severed by the process of making a subdivision. Highly detailed simulations are usually concerned with such subdivisions.

%There is a core of models of realistic models who are missing a substantial number of "extrinsic" synaptic inputs. 
\item 
\item Almost all cortical neurons experience "tonic" synaptic input and these tonic inputs originate from neurons from a different part of the brain. One strategy for handling inputs that are external to the region of interest, is to simply model spike trains for each input synapse. Modern programming languages have tools that can make matching the synthesis of statistically similar spike trains convenient. One big problem with this approach is it assumes that post synaptic neurons are mainly influenced by the firing rate of inputs, if they pre synaptic neurons are actually conveying important code words via exact interspike intervals, a statistical approach to modelling spike trains would not do.
\item 
\item  An alternative approach is to reduce the complexity of neural models as one exceeds the boundary of the region of interest.

%than generating only psuedo random timed inputs to synapses, 
In the case of the Allen Institute Model, if the region of interest V1 is a "core" of realistic neurons. That is a kernel of realistic neurons encased by a shell of less realistic neurons. Inputs to V1 also come from the outer encasement of neurons. It is therefore of interest if these external GLIF models can or should be substituted with optimized Izhikivich and Adexp models, in case substiting GLIF for Adexp results in an overall more realistic network simulation. that even the external shell of the simulation is more realistic. In network models there are benefits of reduced models compared to merely simulating spike train inputs. One of these benefits is that the firing of reduced neural models can be made to be causal, such that its spike times are not just what statistically matches missing models. Furthermore reduced models can still participate in networks, reduced models can become disconnected or participate in an dynamic assembley. Realistic levels of plasticity of the modelled network is more possible with included reduced models, than statistical surrogates of those models.

Furthermore Izhikivitch, GLIF and AdExp models are commonly utilized in neuromorphic archictecture.

\subitem optimization is an interaction between models and constraints which guides a fitting process. Not all neural models are equally flexible.  
\item  Both the choice of constraining equations, and the choice of neural models must be favorable in order for models to be fitted to data.
\item 
\subitem if the combination of models and constraints is bad, then then a tractible error surface will not result.  

\subsubitem Unfortunately, it is not always possible to know without trying which combinations of \subitem[A]: neural models, and \subitem[B], constraints will lead a tractable error surface that a Genetic Algorithms can use to find a global solution to. As an example consider  

I describe some code implementation experiments were the model/constraint combination lead to DEAP genetic algorithms matching model parameters to constraints and model/constraint selections that lead to optimizer performing only marginally better than a random search of parameter space\\
\\
\item If the number of dimensions that are searched exceeds the degree and the effectiveness of constraints, then model optimization is only slightly better than random sampling of solution space.

\item \subsection{Successful Optimization} is more likely to come about by a good selection of models and objective function combinations. 
\item How model constraint combinations interact cannot always be known in advance, and the interaction has to be explored experimentally. Experimenting of model test combinations is what was done in this body of work.

Efficient model examples: (generalized leaky integrate and fire model) GLIF, Izhikitch. Adaptive Exponential Integrate and fire model, single compartment conductance based model. 

\item You could make biophysically accurate models faster, or you could make reduced models more accurate. To make reduced models more accurate, you would find parameterizations of the models that let the models act as better mimics of experiments.
\item Herein we investigate how well Faster models can match experimental recording waveform shapes.
\item The reason why we want to investigate the match:
\item Large scale simulations cant evaluate on a timescale that is meaningful, unless a large ratio of modelled cells are "reduced models"
\item Reduced Models already enjoy wide spread usage. We want to investigate if reduced models can be made to be more realistic, by checking if they can mimic data better.
\item If reduced models can't be made more realistic (herein we show only marginal improvements), we need to show the limitations of reduced cells, with regards to a particular set of tests.
\item We need to document the approach used, and how the approach contrasts with spike time approaches to model fitting.
\end{itemize}

\subsection{Optimizing multispiking Behavior}
\begin{itemize}
\item First with $2$ constraints, then with $>100$ constraints via the feature extraction suite EFEL

\item This is a two staged approach. 
\item It uses three step protocols.

\item In a multispiking paradigm you can see a high variance explained ratio of approx $1$.  

\item In this two stage algorithm: I optimize, using constraints: spike count at injection strength $1.5 \times rheobase $, and $3.0 \times rheobase $
I do a quick check of spike counts at 1.5 and 3.0 rheobase (2 constriants only).

\item In this sceanario Rheobase is used only as a soft constraint. Firstly we optimize the model to spike count data we very rapidly narrow down the solution space using only two errors. \\
\\
In a second code experiment I use the standard NU suite, for a lot of generations. In the figure below
%%
% Figure
%%
Caption in this figure there is visibly almost perfect agreement between simulated and experiments, and the optimized models, in the passive model behavior, and a close match for the spiking model behavior. There was a standard suite of tests if only spike-half-width, not spike-base-width. The Izhi models width as thus free to vary at the base, take that into account when eye balling the two graphs and you can see why almost binary match. EFEL does achieve spike width binary matching because it uses both half-width, and base-width. If you look at the last cells you can see I take a correlation matrix of the optimizers errors over its history. The idea is if it's normalized then I can sum the whole matrix and get a single scaler number to show how de-correlated both error sets are over the GA evolution. 

It was found that the standard suite of NU tests don't fully constrain the "base spike width" of the waveform which is free to vary, half-spike-width is constrained, it is more appropriate to talk about variance explained of the spike snippet. $variance explained>0.95$ is a useful heurism. Allowing some margin acknowledges that we shouldn't assume we have represented all waveform features that can vary. If you want variance explained ==1   you could optimize using a variance explained cost function, but we don't want to do that. 
\end{itemize}

\include{chapters/Introduction}
\chapter*{Methods}
\include{chapters/METHODS_figures}
\include{chapters/METHODS_technical_details_of_the_optimizer}
\chapter*{Results}
\include{chapters/RESULTS_verification_of_the_optimizer}
\include{chapters/RESULTS_large_scale_variance}
\chapter*{Discussion}


\end{document}
