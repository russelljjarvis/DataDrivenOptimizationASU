

%Flat regions of error surface are uninformative.
Tests that the author curated from Allen Cell types led to some models being under-constrained about spike width. The severe consequences of under-restraining were not obvious, they were revealed by graphs in a virtual experiments were appropriate models were elicited to spike. The lack of constraint was easily rectified, by imposing a specific spike width constraint on Adaptive exponential models, however, unexpectedly in this context, the models $\chi^{2}$ increased dramatically and biological plausibility plummeted, in all except one test. To  paraphrase, the adaptive exponential models had found an unexpected way to cheat tests, by taking advantage of a lack of constraints in an unrelated area.



There was a need to create extra constraints for fitting models, as the set of Allen Constraints was too few in extent, and not sufficient to properly constrain models.



In type of standard, the NeuronUnit tests, themselves act as the final judge of model quality, in the absence of a spike width tests, many AdExp models were able to get very good fits on against supplied constraints, but plots of actual spike shape looked very unnatural, as spike width lasted $>=$ 6ms. Applying extra standards beyond the NeuronUnit tests creates a dilemna. As all the GLIF models, presented unusual spike shapes.


\subsubsection{Experiment Fitted Model Results} 

%Allen Brain Institute, Cell-types E-Physiological data, Elephant Tests
%\subsection{Experiment Fitted Results Neuroelectro data, Elephant Tests}

Over four different Allen experimental sweeps, and four different different cell type specific electrical observations, we created eight unique data sets, and then converted these eight data sets to neuronunit test suites.

We then took four different models, and we attempted to fit each test set to each model. The result of was a four $\times $ eight factorial of model data combinations. For each each member of this 32 set factorial we wanted to know if the fitted model behaved in a biological plausible manner, we were interested to know if fitted models were convincing mimics of in-vivo cells, at least with respect to the measurements models were trained to fit.


\subsection{Standard Error of the Mean}
\begin{tabular}{lrrrrrrr}
\toprule
{} &  Rheobase &  SpikeThreshold &  SpikeHalfWidth &  SpikeAmplitude &  MembraneTimeConstant &  RestingMembranePotential &  InputResistance \\
sem                                &           &                 &                 &                 &                       &                           &                  \\
\midrule
Hippocampus CA1 pyramidal cell     &    122.88 &            1.85 &            0.12 &            3.68 &                  3.88 &                      0.72 &            12.57 \\
Olfactory bulb (main) mitral cell  &       NaN &            5.69 &            0.12 &            2.83 &                  5.42 &                      1.39 &            20.17 \\
Cerebellum Purkinje cell           &    419.81 &            2.00 &            0.05 &            0.57 &                   NaN &                      3.69 &            19.26 \\
Neocortex pyramidal cell layer 5-6 &    128.87 &            1.92 &            0.17 &            1.49 &                  2.56 &                      1.84 &            27.93 \\
\bottomrule
\end{tabular}


Since some of the NeuroElectro data sources were more challenging to fit, we also computed the Standard Error of the Mean, so that later we could interprite model fitting failure. Forinstance, the SEM is a prediction of a measurements dispersal, it is often used when the standard deviation is unknown. 

The $\chi^{2}$ statistic, and the p-value give us a formally indicates how convincing each fitted model was in its mimicry of biological measurements.
Because we already have a list of Z-scores, the $\chi^{2} $statistic can be obtained by squaring each Z-score and summing the result. A sum of squares less than 1, tells us that models performed well.
$\chi^{2}=\Sigma_{i} (Zscore_{i}^{2}) $

%https://en.wikipedia.org/wiki/Chi-square_distribution
%This allows you to state this as a hypothesis test with a p-value.  The chi-square statistic would simply be , and the p-value would be 1-scipy.stats.chi2.cdf(x, 8) where 8 is the number of elephant tests (and Z-scores).  A very small p-value (which would come from very large chi-square statistic, much larger than expected for random variation) %would mean the optimizer was less successful at recovering the true model.

See appendix:\ref{table:static_electrical_properties}
The Izhikevich Model and the Point Conductance Model were able to achieve high p-values, and small chi-squared statistics when seven or eight of the tests were considered together.

For example the Izhikevich model fitted to Hippocampus CA1 pyramidal cell data achieved $ (\chi^{2} ,p-value) =(2.1250913868824415, 0.9769347643323284) $

The olfactory Mitral cell
$ (\chi^{2} ,p-value) =(2.0190436240810925, 0.9804224622781068) $

and the cerebellum Purkinje cell. For contrast p-value and chi-squared statistics on the best random models were:

%(2.0190436240810925, 0.9804224622781068)r
\subsection{Data fitted Results on four different classes of model were only modestly good for the Izhikevich Model and the Point Conductance Model}

To make the argument that limitations in reduced neural models were the cause of modest model/experiment agreement, we first had to rule out alternative possibilities. One possibility was that the experimental measurements were faulty, and that the measurements were possibly spurious because of in appropriate averaging. It was found that mainly the olfactory bulb mitral cell was prone to having underlying bi-modal data distributions, and this was especially true for resting membrane

that the models shouldn't be able to reproduce. When considering the data sources, the methods of data collection and data quality should be reliable, the one main 

we first needed to exclude the possibility that there might be something wrong with the data, we are fitting models to.

We needed to rule out was that the data sources did not reflect bi-modal distributions, since the neuroelectro data was the actually the mean over different laboratories, animals, and recording epochs. The mean of a population can often be a robust representation of a population, however there are circumstances when the opposite is true, such as when the underlying data is generated by two different processes, leading to bimodal distributions. Fortunately, it is not hard to test if experimental data, has an underlying bimodal distribution. That test was performed using human judgement for all measurements that were used to fit the model to experiments. Except in the case of the Allen Brain data, because the Allen Brain data consisted of individual raw experiments, and population averaging did not apply. For measurements used in the elephant optimization pipeline.

Below I show distributions for  Time Constant, Input Resistance, Capacitance, Rheobase, Resting Membrane Potential, bimodal distributions did not apply, so we could rule out inaccurate data as a reason for poor model performance.

There is no reason to believe that reduced neural models could not be made to fit inaccurate neural recordings as well as real ones, if the fiction is just caused by noise, then it is still possible that hypothetical spurious values would still be in reach of the Izhikevich model. The izhikevich model can be made to generate some physiologically implausible waveform shapes. 
% a different question, of are the models arbitary waveform generators? 

Besides even if the data was wrong, it wouldn't necessarily follow that models couldn't reproduce the inaccurate data. Instead what we see is, that models can fit one type of experimental measurement at a time, but they can't fit all measurements at once. This result suggests that model flexibility is the most fundamental cause of modest, model experiment disagreement.


%In light of this result, one might ask, i
If reduced models are not excellent at fitting data, are brain simulations really that much more realistic, when we use data driven fitted models in the place of generic model parameters? If data driven model fits lead to models that fit one measurement better than others, which fitness criteria will lead to the greatest consequence for network simulations?

%to answer that question we have created some large




\section{Limitations data we were using was of Existing Approaches} 
Existing community supported simulated models were problem ridden, and our own custom methods were used as work arounds.

% NEURON version of Izhi model is not as fast as one might expect, this may because of the way we tried to implement the model, by creating and destroying HOC module instances, that contain the model. 

%A
% Nonetheless, 
At first we accessed an implementation of the Izhikevich model that was translated from jNEUROML into a NEURON simulator implementation. The problem with this implementation is that it had fragmented the whole Izhikevich model into chattering and non chattering subtypes. The model implementation seemed to have an internal conflict between two capacitive terms. The problem was that the Izhikevich model requires only one capacitive term.

implemented in NEURON which was derived from a J-NeuroML translation, could not reproduce all of the Izhi original publication figures, because it had two different conflicting sources of capacitance.\\
\\
From J-NeuroML we created NEURON version of Izhikich model, however we were not able to make this model execution times brief enough to be useful for optimization. Although the NEURON simulator is designed to be fast, their may be a cost associated with reloading the NEURON environment many times in fast succession.

This should not go in the general introduction, but in the intro to the appropriate section of the results where you use those models.