

%Flat regions of error surface are uninformative.
%Tests that I curated from Allen Cell types lead to some models being under-constrained about spike width. The consequences of  under-restrained were not obvious/

%they were revealed by graphs in a virtual experiments were appropriate models were elicited to spike. The lack of constraint was easily rectified, by imposing a specific spike width constraint on Adaptive exponential models, however, unexpectedly in this context, the models $\chi^{2}$ increased dramatically and biological plausibility plummeted, in all except one test. To  paraphrase, the adaptive exponential models had found an unexpected way to cheat tests, by taking advantage of a lack of constraints in an unconstrained area, ie adopting implausibly long spike widths made the AdExp models exceptionally good at passive tests.

%In type of standard, the NeuronUnit tests, themselves act as the final judge of model quality, in the absence of a spike width tests, many AdExp models were able to get very good fits on against supplied constraints, but plots of actual spike shape looked very unnatural, as spike width lasted $>=$ 6ms. Applying extra standards beyond the NeuronUnit tests creates a dilemna. As all the GLIF models, presented unusual spike shapes.


There was a need to create extra constraints for fitting models, as the set of Allen Constraints was too few in extent, and not sufficient to properly constrain models.

%
%Although the Rheobase was the cause of  significantly impeding error, it was not so much of a problem to include this test itself, it was more of a problem to include tests that where contingent on its value.

%error which could propogate into other tests that depend on its value, 


% From the plots below one can see that 



\subsection{Experiment Fitted Model Results} 

%Allen Brain Institute, Cell-types E-Physiological data, Elephant Tests
%\subsection{Experiment Fitted Results Neuroelectro data, Elephant Tests}

Over four different Allen experimental sweeps, and four different different cell type specific electrical observations, we created eight unique data sets, and then converted these eight data sets to neuronunit test suites.

We then took three different models, and we attempted to fit each test set to each model. The result of was a three $\times $ eight factorial of model data combinations. For each each member of this $24$ set factorial we wanted to know if the fitted model behaved in a biological plausible manner, we were interested to know if fitted models were convincing mimics of in-vivo cells, at least with respect to the measurements models were trained to fit.

\subsection{Standard Error of the Mean}
\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrrr}
\toprule
{} &  Rheobase &  SpikeThreshold &  SpikeHalfWidth &  SpikeAmplitude &  MembraneTimeConstant &  RestingMembranePotential &  InputResistance \\
Neuron Type                              &           &                 &                 &                 &                       &                           &                  \\
\midrule
Hippocampus CA1 pyramidal cell     &    122.88 &            1.85 &            0.12 &            3.68 &                  3.88 &                      0.72 &            12.57 \\
Olfactory bulb (main) mitral cell  &       NaN &            5.69 &            0.12 &            2.83 &                  5.42 &                      1.39 &            20.17 \\
Cerebellum Purkinje cell           &    419.81 &            2.00 &            0.05 &            0.57 &                   NaN &                      3.69 &            19.26 \\
Neocortex pyramidal cell layer 5-6 &    128.87 &            1.92 &            0.17 &            1.49 &                  2.56 &                      1.84 &            27.93 \\
\bottomrule
\end{tabular}
}
\caption[Standard Error of the mean across NeuroElectro Data Sources]{A table of SEM values that describe dispersal in all of the different measurements I used to fit the optimized models to.}
\end{table}

Since some of the NeuroElectro data sources were more challenging to fit, we also computed the Standard Error of the Mean, so that later we could interpreted model fitting failure. Like variance, the SEM is a prediction of each measurements dispersal, it is more theoretically sound that a standard deviation measurement. Without knowing the true underlying data distribution, one can not know the true standard deviation, but it is possible to estimate the standard error of the mean. 

The $\chi^{2}$ statistic, and the p-value give us a formally indicates how convincing each fitted model was in its mimicry of biological measurements.
Because we already have a list of Z-scores, the $\chi^{2}$ statistic can be obtained by squaring each Z-score and summing the result. A sum of squares less than 1, tells us that models performed well.
\begin{equation}
\chi^{2}=\Sigma_{i}^{n} (Z_{i}^{2})
\end{equation}

%https://en.wikipedia.org/wiki/Chi-square_distribution
%This allows you to state this as a hypothesis test with a p-value.  The chi-square statistic would simply be , and the p-value would be 1-scipy.stats.chi2.cdf(x, 8) where 8 is the number of elephant tests (and Z-scores).  A very small p-value (which would come from very large chi-square statistic, much larger than expected for random variation) %would mean the optimizer was less successful at recovering the true model.

%See appendix:\ref{table:static_electrical_properties}
The Izhikevich Model and the Point Conductance Model were able to achieve high p-values, and small chi-squared statistics when seven or eight of the tests were considered together.
For example the Izhikevich model fitted to Hippocampus CA1 pyramidal cell data achieved ($\chi^2$, p-value) = (2.13, 0.98)
The Olfactory Bulb Mitral cell achieved ($\chi^2$, p-value) = (2.02, 0.98).
An extremely high p-value (such as those above) is evidence that the optimized model is much closer the mean of the data distribution than a random experimental neuron.
This is exactly the result one would expect from successful optimization.

\subsection{Results for four model classes}
%were only modestly good for the Izhikevich Model and the Point Conductance Model}

To make the argument that limitations in reduced neural models were the cause of modest model/experiment agreement, we first had to rule out alternative possibilities. One possibility was that the experimental measurements were faulty, and that the measurements were possibly spurious because of in appropriate averaging. It was found that mainly the olfactory bulb mitral cell was prone to having underlying bi-modal data distributions, and this was especially true for resting membrane

that the models shouldn't be able to reproduce. When considering the data sources, the methods of data collection and data quality should be reliable, the one main 

we first needed to exclude the possibility that there might be something wrong with the data, we are fitting models to.

We needed to rule out was that the data sources did not reflect bi-modal distributions, since the neuroelectro data was the actually the mean over different laboratories, animals, and recording epochs. The mean of a population can often be a robust representation of a population, however there are circumstances when the opposite is true, such as when the underlying data is generated by two different processes, leading to bimodal distributions. Fortunately, it is not hard to test if experimental data, has an underlying bimodal distribution. That test was performed using human judgement for all measurements that were used to fit the model to experiments. Except in the case of the Allen Brain data, because the Allen Brain data consisted of individual raw experiments, and population averaging did not apply. For measurements used in the elephant optimization pipeline.

Below I show distributions for  Time Constant, Input Resistance, Capacitance, Rheobase, Resting Membrane Potential, bimodal distributions did not apply, so we could rule out inaccurate data as a reason for poor model performance.

There is no reason to believe that reduced neural models could not be made to fit inaccurate neural recordings as well as real ones, if the fiction is just caused by noise, then it is still possible that hypothetical spurious values would still be in reach of the Izhikevich model. The izhikevich model can be made to generate some physiologically implausible waveform shapes. 
% a different question, of are the models arbitary waveform generators? 

Besides even if the data was wrong, it wouldn't necessarily follow that models couldn't reproduce the inaccurate data. Instead what we see is, that models can fit one type of experimental measurement at a time, but they can't fit all measurements at once. This result suggests that model flexibility is the most fundamental cause of modest, model experiment disagreement.


%In light of this result, one might ask, i
If reduced models are not excellent at fitting data, are brain simulations really that much more realistic, when we use data driven fitted models in the place of generic model parameters? 

% To answer this question we would need to run brain simulations. From my own experience adding in realistic cells to pre-existing network topologies requires that the network be re-tuned to demonstrate tonic firing with realistic CV again.

If data driven model fits lead to models that fit one measurement better than others, which fitness criteria will lead to the greatest consequence for network simulations?



%to answer that question we have created some large

%\subsection{Limitations of Existing Approaches} 
%Existing community supported simulated models were problem ridden, and our own custom methods were used as work arounds.
%data we were using was
% NEURON version of Izhi model is not as fast as one might expect, this may because of the way we tried to implement the model, by creating and destroying HOC module instances, that contain the model. 

%A
% Nonetheless, 

% This should not go in the general introduction, but in the intro to the appropriate section of the results where you use those models.