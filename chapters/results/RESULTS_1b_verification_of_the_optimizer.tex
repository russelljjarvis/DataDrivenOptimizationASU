\section{Verification of the Optimizer}
\label{sec:optimizer-verification}
The reduced models I used are known to be too simple to precisely match all electrophysiological features exhibit by all cell types; this can be an advantage, as this also means that they are too inflexible to fit the ``noise" in the data, whether it be random thermal fluctuations or systematic recording errors.
These latter may be quite common; sites like NeuroElectro control only the extracted data is faithful to that reported in the publication, not that it reflects what was seen in the actual experiments, or that those experiments were carefully performed.
Therefore, if the first step were applying the optimizer to fit models to real biological data, we would have no idea whether any optimization failures (poorly fitting models) reflected the limitations of the biological data, the limitations of the underlying model, or the limitations of the optimizer algorithms.
%And before attempting to identify the magnitude or cause of poor fits, at least four factors that shape optimizer validation and design must be controlled for.
%These are: the informativeness of measurement errors, the performance of the optimizer, the quality of the data, and the quality of the models.

It is therefore necessary to create data-source independent ``ground truths", by simulating data from the models themselves, and using the optimizer to see if those model parameters can be recovered from that simulated data.
Because genetic algorithms (such as the DEAP \citep{DEAP_JMLR2012} implementation used here) has notably good performance in handling complicated error surfaces (such as Rastrigrin's function), I expect that my optimization frame work, which derives from it, should also be able to handle potentially complicated features spaces such as one would expect from the output of a complex dynamical system like a neuron.

I wrote an algorithm that senses the edges of defined model parameter boundaries, and defines uniformly distributed random numbers within those boundaries. Alternatively I could have chosen to draw numbers from uniformly distributed PRNGs, but doing so would risk testing only typical optimization cases and excluding edge cases. In figure (\ref{fig:})
% The algorithm I wrote to choose random model parameters samples from uniform distributions across parameter ranges, and the case above, shows a model in a fringe case.


\subsection{Verification Endpoints}
My optimizer was capable of identifying model parameterizations that nearly perfectly matched the parameters of the ``ground truth" models that generated the constraining simulated data (Figure \ref{fig:optimizer-verification-radar}).
Simulated output of the two models matched closely as well (as expected from similar model parameters) (Figures \ref{fig:optimizer-verification-traces-1}, \ref{fig:adexp_model_rebound_spike}, \ref{fig:optimizer-verification-traces-3}).
NeuronUnit-based electrophysiological features extracted from the traces shown in those figures, and from other traces simulated in the course of optimization, also showed a near-perfect match (i.e. Z-score of 0, indicating perfect optimization) (Table \ref{table:optimizer-verification}).

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{llll}
\toprule
{} &    observations &     predictions & Z-Scores \\
\midrule
RheobaseTest         &         1.62 pA &         1.62 pA &        0 \\
TimeConstantTest     &        13.18 ms &        13.18 ms &        0 \\
RestingPotentialTest &       -77.43 mV &       -77.43 mV &        0 \\
InputResistanceTest  &  270.84 megaohm &  270.84 megaohm &        0 \\
CapacitanceTest      &        48.65 pF &        48.65 pF &        0 \\
FITest               &    7.51 Hz/pA &    7.51 Hz/pA &        0 \\
\bottomrule
\end{tabular}}
\caption[Optimizer verification table]{``Observed" and ``Predicted" electrophysiological features match nearly perfectly.
Generally, ``observed" would refer to observations taken from biological data, but here it refers to observations from simulations of the ground truth model using various stimuli.
Each test extracts an electrophysiological features from some stimulus.
``Predicted refers to the predicted values of these features from the optimized model.
Z-Scores of 0, self-evident from the equality of the first two columns, indicate perfect optimization for this set of features.}
\label{table:optimizer-verification}
\end{table}

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{figures/radar_coordinates.png}
    \caption[Polor Plot of Optimizer Verification against Ground Truth]{\textbf{Radar Plot of Optimizer Verification against Ground Truth.} This radar plot shows the values of each parameter from the AdEx model for a randomly-chosen target parameterization (red) and the optimizer-discovered optimum parameterization (gray). 
    The optimizer did not observe the parameter values in red directly, instead it used error guidance described above to ``recover" the parameters, as follows: Before optimization output features of the red model, i.e. it simulated membrane potential traces using the parameters shown in red and found that the parameters shown in gray produce traces that were similar on some features encoded in NeuronUnit tests.
    
    The result of this process is two almost identical sets of parameters.
    Out of the 11 parameters in this model, there is only a trivially small discrepancy in $C_{M}$, $v-reset$ and $v-spike$.
    }
    \label{fig:optimizer-verification-radar}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.85]{figures/simulated_data_supra_threshold.png}
    \caption[Optimizer Verification Example 1]{\textbf{Optimizer Verification Example 1.} The orange trace shows the model waveform recovered during the process of optimization. Blue trace (not visible due to occlusion by orange trace) depicts the source of the electrical measurements that were used to guide optimization.
    Figure shows agreement in simulated response between the ground truth model and optimized model from Figure \ref{fig:optimizer-verification-radar}. Both the ground truth model and the optimized model are depicted for rheobase current injection values.}
\label{fig:optimizer-verification-traces-1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.85]{figures/simulated_data_sub_threshold.png}
    \caption[Optimizer verification example 2]{\textbf{Optimizer Verification Example 2.}
    In this plot the blue trace represents the ground truth model, and the green trace represents the best solution obtain via optimization.
    \ref{fig:optimizer-verification-traces-1}. Here I used a constant negative current injection value of $-10pA$, applied between $100ms-600ms$. Because model parameters are assigned to the ground truth model randomly, an unsually small value of 'C' in the AdEx model has lead to the occurrence of multiple rebound bursts, after the inhibitory current was applied.
    
    This demonstrates that the match between the ground truth model and the optimized model is not limited to a single stimulus condition or exclusively to only typical model parameters.}
    \label{fig:adexp_model_rebound_spike}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.85]{figures/passive_model_agreement}
    \caption[Optimizer verification example 3]{
    This plot follows the same color coding conventions to the plot above. The main purpose of this plot, is to show similar sorts of results but in a different model type. As stated in the plot above an unusual value of capacitance was randomly selected as a model parameter in the AdEx model. In the current plot a more conventional set of model parameters were randomly sampeled in the Izhikevich model. The addition of this extra plot is to better represent typical
    model behavior under $10pA$ current injection.   
        
    \ref{fig:adexp_model_rebound_spike} 
    
    The same $-10.pA$ current injection stimulus is applied, this time in the Izhikevich model. 
    The model with the blue trace does rebound visibly by an additional $\sim0.1 mV$, reflecting a limitation of the features used for optimization to distinguish changes that are a) so small and b) irrelevant to the features that most physiologists would care about.
    Obtaining a near-perfect match on a detail such as this would require introducing a new test that measures rebound depolarization after hyperpolarization.}
    \label{fig:optimizer-verification-traces-3}
\end{figure}

In these results, only a small number of features were used (Time constant, Capacitance, Rheobase, Resting Potential and Input Resistance, and FI; interestingly only two of these involved a measurement of action potentials.
Other NeuronUnit tests that measure and judge models according to action potential \emph{width, amplitude, or threshold} did not participate in guiding optimization, because in the model-testing frame-work that I inherited when I entered the ICON laboratory the tests: \emph{width, amplitude, or threshold} where all contingent on variable, per-model rheobase current injection values.
When spike shape measurements are contingent on variable rheobase values, small rheobase approximation errors get amplified confounding the final high resolution stage of genetic algorithm search. Ordinarily this would not preclude achieving reasonable optimisation results, but this context is different. When seeking to recapitulate a ground truth very high precision of results is paramount, therefore I eschewed several possible tests of spike shape that amplify rheobase approximation errors, because unless they were re-written to use fixed current values they could not give me the level of precision I needed.

%% No, this is not the reason.
% because many reduced models simply cannot produce realistic looking waveforms without imputation -- currently false, actually AP shape is fine in these versions of Izhikevich and Adex and they have been shown to achieve experimental agreement in those measurements.
% The reason is the corrogations/ripples I have been talking about for the last while.

The AdExp model, for example, is far from an arbitrary waveform generators.
Results for additional models and parameterizations are given in the Appendix.

Failure of optimization verification, when it occurs, could be indicative of insufficient constraints.
By analogy, when solving a system of linear equations, finding a unique solution requires that the number of constraining equations is greater than the number of free variables you are solving for.
Similarly, in a genetic optimization algorithm we solve for unknown variables using stochastic principles, but the number of variables (i.e. model parameters) we can identify is still limited by the number of independent measurements of model output that we use.
In other words, assuming that the number of objectives in the multiobjective optimization problem, $NOBJ$ is greater than $NDIM$, the number of model parameters, optimization should be achievable.
Even when $NOBJ<NDIM$, such as in the examples above, optimization can still work due to correlations or redundancies in the model that lead the actual manifold on which models live to be of lower dimension than $NDIM$ itself.

\subsection{Verification Efficiency}
My optimizer can recover ground truth models from simulated data in several cases.
Does it do so efficiently?
How long does this optimization take, and does it get stuck exploring irrelevant regions of parameter space?

In Figure \ref{fig:optimizer-evolution}, I show how the optimized model converges towards the ground truth model over time.
In can require up to $200$ generations of parameter set evolution for tight convergence to be realized.
This takes approximately $10-20$ minutes on a my personal laptop.

% I have not done this (since numba on Adex, and IZHI existed) so I don't know the times for HPC. can be accelerated $XXXX$-fold on HPC architecture. 

%\begin{comment}
%\begin{figure}
%    \centering
%  \includegraphics{figures/simulated_data_stats.png}
%    \caption{Optimizer evolution, green line tracks evolution of best fitness, blue line average fitness, %orange line is worst fitness. GA params, $NGEN=200$, $MU=50$,$cxp=0.3$,$mupb=0.2$ from this plot can see that %genes are storing and exploiting information, $cxp+mutpb=0.5$, so $50\%$ of genes are conserved between %generations }
%    \label{fig:my_label}
%\end{figure}
%\end{comment}

\begin{figure}
    \centering
    \includegraphics[scale=0.7]{figures/optimizer_internal_validation}
    \caption[Optimizer error over generations]{Evolution of optimized model quality over generations.
    The green line tracks the lowest error in each generation of candidate models, the blue line shows the average error, orange line the highest error.
    Optimization hyperparameters were: $Number of Generations=200$, $Population Size=50$, $Crossover Probability=0.3$, $Mutation Probability=0.2$.
    The periodic jumps in the orange error represent mutation or crossover events in which poor-performing models were generated.
    These were not typically selected into the subsequent generation.
    Because the crossover and mutation probabilities only sum to 0.5, specifically (0.3cxpb, and 0.2mutpb), the remaining half of all parameter sets in each generation either filtered out by selection or, are simply carried over to the subsequent generation.
    Extensive hyperparameter tuning showed that this level of chromosome conservation was a good balance between exploration and exploitation (not shown.}
    \label{fig:optimizer-evolution}
\end{figure}

\subsection{Alternatives for Verification}
The ground truth is known independently of the optimizer, so one can test alternative strategies to see if there is a solution that represent a better match to ground truth than the one obtained through optimization.
For example, one can exhaustively search the solution space to look for the best model.
An exhaustive search might be a reasonable (but lazy) approach when there are only a small number of free parameters, however using a sampling grid of 25 distinct parameter values for each of N parameters mean that $25^{N}$ total models must be examined.
Even for a simple model ($N=5$) this represents nearly 10,000,000 model evaluations, and of course the problem gets much worse with each additional parameter.
25 values for each parameter may also be insufficient resolution when the parameter regime exhibiting the desired behavior is narrow.

\subsection{Implications of a verified optimizer}
Success here suggests that for reduced models, there is unlikely to be much degeneracy in model parameters, with distinct combinations of parameters producing identical simulated responses across a range of stimuli.
% Actually I did find some degenerecy, but I didn't find a way to systematically quantify it, because I spent too much time confounded by the ripple in surface problems.
This might also be expected from the motivation of reduced models, which is to identify and consolidate redundant or unimportant biophysical equations/parameters into a handful of key reduced model equations/parameters.
Importantly, I showed that the electrophysiological features used here, which correspond to measurements that can be made in real neurons, were sufficient for this task.

Confident that the optimizer can identify model parameterizations that can generate observed electrophysiological features \emph{in principle}, the focus of the remaining sections becomes the localization of other potential sources of model/data disagreement. 
