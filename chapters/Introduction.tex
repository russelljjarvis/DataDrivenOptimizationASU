
\section{Introduction}

\subsubsection{Motivation}
\begin{itemize}
Need to explain two things:
- why optimize, but also why reduced models. Probably why models first.

\item Biophysically accurate models take a significant time and resources to evaluate. A different class of neuronal model, known as a "Reduced" model is comparatively fast to solve especially at a large scale. Speed of simulation is important for learning about the brain. Even in the many valid instances when the complete 3D shape is an integral part of a cortical brain simulation, such simulations are often enriched, by encasing a "core" of biophysically accurate models inside a "shell" of simple fast and reduced Izhikitch, GLIF, or adaptive exponential \cite{brette2005adaptive} Wulfram Gerstner models. Further more Izhikivitch, AD

\section{Introduction}

\subsection{Model  Optimization with NeuronUnit}
Some neural properties can’t be easily measured in experiments. These unknown properties hamper modeling accuracy and require parameter fitting. For example, a common approach for approximating unknown ion channel densities is to ‘optimize’ the governing equations to match known waveforms. The process of optimization involves what is known as an ‘inverse’ problem where we efficiently and sparsely search for the ‘optimal’ value of an parameter that satisfies the system of equations. Often an optimal value is corresponds to a global minimum or maximum value of a cost function.\\*

Computational optimization techniques are generally specific to a particular type of problem rather than being generalized. However, several notable algorithms have solved a wide range of problems including genetic and algorithms and stochastic gradient descent (SGD). The popularity of these two algorithms is due to their robustness. NSGA2 and SGD are able to avoid falsely reporting a local minimum when a more optimal solution is available. 
\Subsection{Multiobjective optimization} Multi objective optimization problems are a subset of optimization problems, where model fitness is evaluated against multiple independent constraints, rather than just one error. It is often possible to reduce multiple constraints into one constraint by summing the outputs of objective functions together, however the price of reducing multiple errors into one error, is that where multiple and diverse models give satisfactory solutions to the provided constraint, reducing error leads to a situation where a constraint that is easier to satisfy, rapidly drags down the error score and dominates the by contributing lower errors to the sum of error scores.

However, of SGD and NSGA2, only NSGA2 is a natural choice for tackling multi-objective optimization problems. Default implementations of SGD are not able to utilize the principle of non-domination as an optimization strategy.%%
%%
There is a great diversity of real biological neurons, all of which differ substantially in their electrical behavior. There are a few different classes of general purpose neuronal models, that can reproduce these different types of electrical behaviours, given appropriate parameterizations of the models.\newline
\newline
An exisiting class of neuron model type, called The Izhikevich model was published with parameter sets believed to make the model outputs accurately align with a variety of real biological cell outputs. However since publication much very specific electro physiological recordings have accumulated, that in someways undermine model/experiment agreement. However it is now possible to constrain the Izhikevich model and find new parameterizations that more allow us to more accurately reproduce more recently published experimental data.\newline
\newline
NeuronUnit easily converts a quantitative measure of model/data agreement into a useful error signal. A very natural application of this signal is to guide the process of optimization. We have used Neuronunit to guide optimization by taking a flexible model type such as a generalized linear integrate and fire model or the Izhikevich model and constraining the model against relevant experimental data. As an example, NSGA2 was used to optimize models in conjunction with data driven tests based on pooled data from NeuroElectro.org. A variety of compact and fast single compartment models were used to explore model optimization. Figure 4 demonstrates test error at the beginning of the optimization process for models with randomly sampled parameters and the smaller error following optimization. Figure 5 shows the evolution of the error during the optimization process. \newline
\newline
Optimized neuron models may vary from their neuron counterparts for several reasons. Table 3 shows an example where optimizing the model with respect to the rheobase test comes into conflict with minimizing with respect to input resistance. The solution to the optimization problem consists of two sets of model parameters, which can resolve this conflict differently. Examining the experimental data that these tests were derived from suddenly becomes important. By examining the data, we can see if the rheobase currents and the distributions of input resistance are bi-modal and uniformly distributed. If the data is treated as uni-modal, and the uni-modal mean is used to optimize then the model, then the model is not able to satisfy both constraints simultaneously. In this case, the measurements don’t correspond to neuron data, and the model can’t produce the artificial behavior. When comparing complex data and simple models we find that solutions are better represented using a combination of two optimization solutions.\newline
\newline


\subitem optimization is an interaction between models and constraints which guides a fitting process. 

\subitem if the combination of models and constraints is bad, then then a tractible error surface will not result.  

\subsubitem Unfortunately, it is not always possible to tell without trying which combinations of A: neural models, and B, constraints will lead to the Genetic Algorithms ability to converge on around the minimal error. 


I describe some code implementation experiments were the model/constraint combination lead to DEAP genetic algorithms matching model parameters to constraints and model/constraint selections that lead to optimizer performing no better than a random search of parameter space.


\item successful optimization means model objective choices. How model constraint combinations interact cannot always be known in advance, and the interaction has to be explored experimentally.

Efficient model examples: (generalized leaky integrate and fire model) GLIF, Izhikitch. Adaptive Exponential Integrate and fire model, single compartment conductance based model. 

\item You could make biophysically accurate models faster, or you could make reduced models more accurate. To make reduced models more accurate, you would find parameterizations of the models that let the models act as better mimics of experiments.
\item Herein we investigate how well Faster models can match experimental recording waveform shapes.
\item The reason why we want to investigate the match:
\item Large scale simulations cant evaluate on a timescale that is meaningful, unless a large ratio of modelled cells are "reduced models"
\item Reduced Models already enjoy wide spread usage. We want to investigate if reduced models can be made to be more realistic, by checking if they can mimic data better.
\item If reduced models can't be made more realistic (herein we show only marginal improvements), we need to show the limitations of reduced cells, with regards to a particular set of tests.
\item We need to document the approach used, and how the approach contrasts with spike time approaches to model fitting.
\end{itemize}